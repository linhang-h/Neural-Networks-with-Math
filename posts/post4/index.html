<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/Neural-Networks-with-Math/libs/katex/katex.min.css"> <link rel=stylesheet  href="/Neural-Networks-with-Math/css/franklin.css"> <link rel=stylesheet  href="/Neural-Networks-with-Math/css/style.css"> <link rel=icon  type="image/png" sizes=32x32  href="/Neural-Networks-with-Math/assets/images/network.png"> <title>Introduction to G-CNNs</title> <header> <nav> <div class=nav-content > <div class=logo > <a href="/Neural-Networks-with-Math/">Neural Networks with Math</a> </div> <ul class=nav-links > <li><a href="/Neural-Networks-with-Math/">Home</a> <li><a href="/Neural-Networks-with-Math/menu1">Posts</a> <li><a href="/Neural-Networks-with-Math/people">People</a> </ul> </div> </nav> <script> let nav = document.querySelector("nav"); window.onscroll = function() { if(document.documentElement.scrollTop > 20){ nav.classList.add("sticky"); }else { nav.classList.remove("sticky"); } } </script> </header> <div class="container py-3 px-3 mx-auto"><div class=franklin-content > <h1 id=group_equivariant_neural_networks ><a href="#group_equivariant_neural_networks" class=header-anchor >Group Equivariant Neural Networks</a></h1> <p>This webpage consists of a set of notes we created while studying the course <em>An Introduction to Group Equivariant Deep Learning</em> by Erik Bekkers at the University of Amsterdam. The course materials are freely available online at <a href="https://uvagedl.github.io/">uvagedl</a>, and the lecture series is available as a public <a href="https://www.youtube.com/playlist?list&#61;PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd">playlist</a> on YouTube. We owe our deepest gratitude for <em>Dr.</em> Erik Bekkers and all other people who created this wonderful course and made the materials publically available. We largely follow the structure of these lectures, even though some of the mathematical details are our own takes on the original arguments. The intrigued reader is highly recommended to check out the original lectures, where concepts are illustrated with many amazing graphics. We hope to duely justify the slogan &#39;<em>Group convolution is all you need</em>&#39; through this short digest. </p> <p>Picture credit: <a href="https://www.frassek.org/3d-fl&#37;C3&#37;A4chen/spherical-harmonics/">Dr. Bernd Frassek</a></p> <div class=franklin-toc ><ol><li><a href="#introduction">Introduction</a><li><a href="#g-cnn_the_first_class">\(G\)-CNN: The First Class</a><ol><li><a href="#convolutional_neural_networks">Convolutional Neural Networks</a><li><a href="#group_equivariant_cnns">Group Equivariant CNNs</a></ol><li><a href="#representation_theory_and_harmonic_analysis_on_lie_groups">Representation Theory and Harmonic Analysis on Lie Groups</a><ol><li><a href="#semi-direct_products">Semi-direct Products</a><li><a href="#lie_groups_and_homogeneous_spaces">Lie Groups and Homogeneous Spaces</a><li><a href="#integration_on_lie_groups_and_homogeneous_spaces">Integration on Lie Groups and Homogeneous Spaces</a><ol><li><a href="#haar_measure">Haar measure</a></ol><li><a href="#the_peter-weyl_theorem_and_harmonics">The Peter-Weyl Theorem and Harmonics</a><li><a href="#g-convolutions_and_architecture_of_g-cnns">\(G\)-Convolutions and Architecture of \(G\)-CNNs</a></ol><li><a href="#g-cnn_regular_vs_steerable_networks">\(G\)-CNN: Regular v.s. Steerable Networks</a><ol><li><a href="#regular_g-cnns">Regular \(G\)-CNNs</a><li><a href="#steerable_g-cnns">Steerable \(G\)-CNNs</a><li><a href="#g-activation_layers">\(G\)-Activation Layers</a></ol><li><a href="#references">References</a></ol></div> <h2 id=introduction ><a href="#introduction" class=header-anchor >Introduction</a></h2> <p>Convolutional Neural Networks &#40;CNNs&#41; possess the distinguishing property of translation invariance in their convolutional layers, allowing them to maintain the inherent spatial structure of image data. This characteristic enables CNNs to excel in various complex tasks, including edge detection, feature extraction for object recognition, and semantic segmentation.</p> <p>However, image data often come with additional structures, such as rotational or reflectional symmetries, which are not built-in symmetries of traditional CNNs. Group-equivariant CNNs &#40;\(G\)-CNNs&#41; extend the translation-invariance property of traditional CNNs into equivariance of more general symmetry groups, such as rotations, reflections, or scaling transformations. This is achieved by defining convolutional layers over topological groups rather than the usual vector space \(\mathbb{R}^n\). As a result, \(G\)-CNNs efficiently represents and processes data with a lot of symmetries, in many cases improving the performance margin significantly. One of the first applications where \(G\)-CNNs are extremely useful is medical imaging, where cells and organs can appear in various orientations &#40;&#91;Bekkers et.al. 2018&#93;&#41;. </p> <p>&#33;&#33;&#33;TBA</p> <h2 id=g-cnn_the_first_class ><a href="#g-cnn_the_first_class" class=header-anchor >\(G\)-CNN: The First Class</a></h2> <p>Say we want to build a network which recognizes the letter \(G\). Regardless of where we position the \(G\) in our input image, we want the feature map of the network to activate in the same way where it detects the \(G\) somewhere in the image. This sort of <em>translation equivariance</em> is a defining feature of Convolutional Neural Networks &#40;CNNs&#41;, and in fact what makes it so useful in pattern recognition tasks, especially in image and spatial data. </p> <h3 id=convolutional_neural_networks ><a href="#convolutional_neural_networks" class=header-anchor >Convolutional Neural Networks</a></h3> <p>CNNs achieve translation invariance through the structure of their convolutional layers. Mathematically, a convolutional layer takes in a signal \(f:\mathbb{R}^n\to \mathbb{R}\) and computes a feature map \(\hat{f}(x)\) as </p> \[\hat{f}(x):=(\kappa * f)(x) = \int_{y\in\mathbb{R}^n} \kappa(x-y) f(y) \, dy\] <p>, where \(\kappa\) is the convolution <em>kernel</em> &#40;aka. filter&#41;. If the signal \(f\) is translated by a vector \(t \in \mathbb{R}^n\), say \(f^\prime(x) = f(x + t)\), then the resulting feature map shifts correspondingly: </p> \[\begin{aligned} \hat{f}^\prime(x) & = (\kappa * f')(x) \\ & = \int_{\mathbb{R}^n}\kappa(x-y)f(y+t)dy \\& = \int_{\mathbb{R}^n}\kappa((x-t)-y)f(y)dy \\& = \hat{f}(x-t). \end{aligned}\] <p>This can be organized into a commutative diagram: </p> \[\begin{tikzcd} f & {\hat{f}} \\ {f^\prime} & {\hat{f}^\prime} \arrow["{\tiny{\text{ convolution }}}"{marking, allow upside down}, from=1-1, to=1-2] \arrow["{\tiny{\text{ translation }}}"{marking, allow upside down}, from=1-1, to=2-1] \arrow["{\tiny{\text{ translation }}}"{marking, allow upside down}, from=1-2, to=2-2] \arrow["{\tiny{\text{ convolution }}}"{marking, allow upside down}, from=2-1, to=2-2] \end{tikzcd}\] <p>In plain words, <em>convolving first and then translating results the same as translating first and then convolving</em>. This demonstrates that the convolution operation is <em>compatible with translational symmetries</em> and is exactly what makes CNNs so good with acoustic and visual signals. </p> <h3 id=group_equivariant_cnns ><a href="#group_equivariant_cnns" class=header-anchor >Group Equivariant CNNs</a></h3> <p>Our physical world is brimming with symmetries – from the discrete polytopal symmetries of a virus to the rotational symmetries of the surface of a planet. </p> <p><em>TODO: add pictures of the icosahedral and SO&#40;3&#41; groups</em></p> <p>It is an inevitable consequence that the data inputted in neural networks come with a lot of built-in symmetries. Respecting, or better, leveraging those symmetries has proven to be the key in speeding up training and improving accuracy. It is therefore requisite that we build network architectures that are <em>compatible with more general symmetries</em>, i.e. groups other than \(\mathbb R^n\). </p> <p>To do this, we ask the generalized convolution layers to satisfy a similar property: </p> <p>&lt;p style&#61;&quot;text-align: center;&quot;&gt; Group convolution followed by group translation should be the same as group translation followed by group convolution. &lt;/p&gt;</p> \[\begin{tikzcd} f & {\hat{f}} \\ {f^\prime} & {\hat{f}^\prime} \arrow["{\tiny{G-\text{ convolution }}}"{marking, allow upside down}, from=1-1, to=1-2] \arrow["{\tiny{G-\text{ translation }}}"{marking, allow upside down}, from=1-1, to=2-1] \arrow["{\tiny{G-\text{ translation }}}"{marking, allow upside down}, from=1-2, to=2-2] \arrow["{\tiny{G-\text{ convolution }}}"{marking, allow upside down}, from=2-1, to=2-2] \end{tikzcd}\] <p>Fix the ground field to be \(\mathbb k\), which for all practical purposes will be \(\mathbb R\) or \(\mathbb C\). &#40;It will be awesome if you are able to come up with an application of \(G\)-CNNs with a more exotic ground field&#33;&#41;</p> <p>So say we are given a function &#40;aka. a signal&#41; \(f\) on some space \(X\) with a large group of symmetries \(G\) –– so large that \(X\) is in fact a <em>homogeneous space</em> of \(G\). &#40;This requirement is so that we have a nice representation theory of \(G\) on the space of functions over \(X\).&#41; A \(G\)-convolution then amounts to a choice of convolution kernel \(\kappa: X \to \mathbb k\) satisfying <em>a certain symmetry constraint</em>, so that the convolution is \(G\)-equivariant. </p> <p>Formally, assume \(X \cong G/H\) for a normal subgroup \(H\), with <em>Haar measure</em> \(d\mu\). We fix a representation \(\rho\) of \(G\) on the space of square-integrable functions \(L_2(X)\),with inner product denoted \(\langle -,-\rangle_X\). For a point \(x \in X\), pick a coset representative \(g_x\in G\) such that \(x = [g_xH]\). Then, a \(G\)-convolution \(\mathcal{K}: L_2(X) \to L_2(X)\) has the form </p> \[(\mathcal{K}f)(x) := \langle \rho(g_x)(\kappa),f\rangle_X = \int_X (g_x\cdot \kappa)fd\mu.\] <p>In fact, it is a theorem of Bekkers that all such equivariant transformations on signals come in the form of a \(G\)-convolution&#33;</p> <div class=note ><div class=title >Theorem &#91;Bekkers ICLR 2020, Thm. 1&#93;</div> <div class=content ><em>All \(G\)-equivariant transformations between signals on homogeneous spaces of \(G\) come in the form of \(G\)-convolutions as above.</em></div></div> <p>This comes with a caviat that the kernel \(\kappa\) must be <strong>invariant under the subgroup \(H\)</strong>, since we made a choice of the representative \(g_x\). Fortunately, we can choose to not worry about this constraint by <em>lifting the convolution</em> to the big group \(G\). </p> <div class=note ><div class=title >Theorem &#91;Bekkers ICLR 2020&#93;</div> <div class=content ><em>All \(H\)-invariant kernels \(\kappa\) come from the projection of some function \(\hat{\kappa}\in L_2(G)\) without symmetric constraints to \(L_2(X)\).</em></div></div> <p>Given such, the general architecture of a \(G\)-CNN is as follows: </p> <ol> <li><p>Start with an input signal on \(X\).</p> <li><p>Lift the input signal to \(G\).</p> <li><p>Convolution layer: \(G\)-convolve with a chosen kernel \(\kappa\). </p> <li><p>Activation layer: filter the transformed signal through a chosen \(G\)-equivariant activation function.</p> <li><p>Repeat steps 3 and 4. </p> <li><p>Project down to \(X\) by max pooling over \(H\). </p> </ol> <p>&#33;&#40;GCNN&#41;&#91;/_assets/images/G-CNN-architecture.png&#93;</p> <p>In the next sections, we are going to build up the necessary representation theory and investigate how such \(G\)-convolution layers can be built computationally. </p> <h2 id=representation_theory_and_harmonic_analysis_on_lie_groups ><a href="#representation_theory_and_harmonic_analysis_on_lie_groups" class=header-anchor >Representation Theory and Harmonic Analysis on Lie Groups</a></h2> <h3 id=semi-direct_products ><a href="#semi-direct_products" class=header-anchor >Semi-direct Products</a></h3> <p>Say we have two groups \(G\), \(H\), and we want to build a bigger group with \(G\) and \(H\) as the coordinate axes. The easiest way to do this is the direct product \(G\times H\). The next simplest thing is a &quot;twisted product&quot; – a <strong>semi-direct product</strong> \(G \rtimes H\). </p> <p>Formally, we require an action \(\rho\) of \(H\) on \(G\). Then, as a set, the semidirect product \(G\rtimes_\rho H\) has the same elements \((g,h)\) as the direct product \(G\times H\), however with an altered group law. Namely, we define a new group product </p> \[(g_1, h_1)\ast (g_2,h_2):= (g_1\rho_{h_1}(g_2), h_1h_2),\] <p>so we see that the \(G\)-coordinate has been &quot;twisted&quot; by the action of \(H\). </p> <p>If the action \(\rho\) is trivial, then we recover the direct product of the two groups. Furthermore, we can classify all possible semidirect products in terms of non-isomorphic actions of \(H\) on \(G\). We call semidirect products a <strong>&#40;split&#41; extension</strong> of \(G\) by \(H\), in the sense that the groups fit into a short exact sequence </p> \[1\to G \to G\rtimes H \to H\to 1\] <p>which splits – resembling a locally trivial bundle in manifold theory. </p> <div class=note ><div class=title >Examples</div> <div class=content ><p>The <strong>special Euclidean group</strong> \(SE(2)\) is an extension of the group of translations \((\mathbb R^2, +)\) of the plane by rotations \(SO(2)\) in the plane. It has matrix representation </p> \[ g=\left(x, R_\theta\right) \quad \leftrightarrow \quad G=\left(\begin{array}{ccc} \cos \theta & -\sin \theta & x \\ \sin \theta & \cos \theta & y \\ 0 & 0 & 1 \end{array}\right)=\left(\begin{array}{cc} R_\theta & x \\ 0^T & 1 \end{array}\right) \] <p>and group law</p> \[ (x, \theta) \cdot\left(x^{\prime}, \theta^{\prime}\right)=\left(R_\theta x^{\prime}+x, \theta+\theta^{\prime} \bmod 2 \pi\right) \] <p>or in matrix form,</p> \[ \left(\begin{array}{cc} R_\theta & x \\ 0^T & 1 \end{array}\right)\left(\begin{array}{cc} R_\theta^{\prime} & x^{\prime} \\ 0^T & 1 \end{array}\right)=\left(\begin{array}{cc} R_{\theta+\theta^{\prime}} & R_\theta x^{\prime}+x \\ 0^T & 1 \end{array}\right). \] <p>The Euclidean group \(E(n)\) and special Euclidean group \(SE(n)\) are similarly defined.</p></div></div> <p>TODO: picture of SE2</p> <p><div class=note ><div class=title >Examples</div> <div class=content ><p>The <strong>scale-translation group</strong> \(\mathbb R^2 \rtimes \mathbb R^\times_+\) is an extension of \(\mathbb R^2\) by the multiplicative group \(\mathbb R^\times_+\). The group law is given by </p> \[ g \cdot g^{\prime}=(x, s) \cdot\left(x^{\prime}, s^{\prime}\right)=\left(s x^{\prime}+x, s s^{\prime}\right) \]</div></div> TODO:picture of scale translation</p> <h3 id=lie_groups_and_homogeneous_spaces ><a href="#lie_groups_and_homogeneous_spaces" class=header-anchor >Lie Groups and Homogeneous Spaces</a></h3> <p>A <strong>Lie group</strong> is a topological group endowed with the structure of a smooth manifold. A transitive group action is an action with a single orbit. A <strong>homogeneous space</strong> for a Lie group \(G\) is then a smooth manifold \(X\) with a transitive \(G\)-action. </p> <p>Say we have a \(G\)-action on \(X\) with just a single orbit. Then, for any point \(x\) in the single orbit \(X\), the orbit-stabilizer theorem says that </p> \[X = G/\mathrm{Stab}_x\] <p>as a set, which can be upgraded to a diffeomorphism of smooth manifolds. Therefore, \(G\)-homogeneous spaces are in 1-to-1 correspondence to quotients \(G/H\) where \(H\) is a normal subgroup. Elements in \(X\cong G/H\) are then <strong>cosets</strong> of \(H\). The stabilizer subgroup \(\mathrm{Stab}_x\) is often called the <strong>isotropy subgroup</strong> of a point \(x\), which consists of all transformations in \(G\) that fix \(x\). </p> <div class=note ><div class=title >Examples</div> <div class=content ><p>Any Lie group \(G\) is a homogeneous space over itself with trivial isotropy. </p> \[G\cong G/\{1\}.\]</div></div> <div class=note ><div class=title >Examples</div> <div class=content ><p>The orthonogal group \(O(n+1,\mathbb R)\) acts on the \(n\)-sphere \(S^n \subseteq \mathbb R^{n+1}\) with isotropy \(O(n,\mathbb R)\). Therefore, </p> \[S^n \cong O(n+1,\mathbb R)/O(n,\mathbb R).\] <p>There is a similar story for the complex \((2n-1)\)-sphere in \(\mathbb C^n\).</p></div></div> <div class=note ><div class=title >Examples</div> <div class=content ><p>The special Euclidean group \(SE(n)\) acts on \(\mathbb R^n\) via all rigid body motions &#40;composites of rotations and translations&#41;. The isotropy of each point is \(SO(n)\), since a point in \(\mathbb R^n\) does not carry the data of an orientation. Therefore, </p> \[\mathbb R^n \cong SE(n)/SO(n).\]</div></div> <div class=note ><div class=title >Examples</div> <div class=content ><p>The group of scaling-translations \(\mathbb R^n\rtimes\mathbb R^\times_+\) acts on \(\mathbb R^n\) composites of translations and scalings. The isotropy of each point is \(\mathbb R^\times_+\), since a point in \(\mathbb R^n\) does not see the scaling. Therefore, </p> \[\mathbb R^n \cong (\mathbb R^n\rtimes\mathbb R^\times_+)/\mathbb R^\times_+.\]</div></div> <div class=note ><div class=title >Examples</div> <div class=content ><p>The orthonogal group \(O(n,\mathbb R)\) acts on the Grassmannian \(Gr_\mathbb R(k,n)\) of \(k\)-planes in \(n\)-space via rotating a \(k\)-plane. The isotropy of each \(k\)-plane is \(O(k)\times O(n-k)\), since each \(k\)-plane is invariant under orthogonal transformations on itself and its orthogonal complement. Therefore, </p> \[Gr_\mathbb R(k,n)\cong O(n)/O(k)\times O(n-k).\] <p>There is a similar story for the complex Grassmannian \(Gr_\mathbb C(k,n)\) and for more general fields \(\mathbb k\).</p></div></div> <h3 id=integration_on_lie_groups_and_homogeneous_spaces ><a href="#integration_on_lie_groups_and_homogeneous_spaces" class=header-anchor >Integration on Lie Groups and Homogeneous Spaces</a></h3> <p>Integration on a manifold is a pairing between formal sums of \(k\)-dimensional submanifolds and differential \(k\)-forms, which returns a scalar in \(\mathbb k = \mathbb R or \mathbb C\). </p> \[\text{ integral } = \int_{\text{ sum of submanifolds } C} \text{ differential form }d\mu.\] <p>For integration over groups &#40;and similarly homogeneous spaces&#41;, we want the number we get to be invariant if we translate our submanifold by any group element \(g\). This leads to the notion of a <strong>left-invariant measure</strong> &#40;resp. right-invariant if the group acts on the right&#41;.</p> <h4 id=haar_measure ><a href="#haar_measure" class=header-anchor >Haar measure</a></h4> <p>Given a \(G\)-space \(X\), a Borel measure \(\mu\) is left-invariant if for any subset \(C\subseteq X\) and any group element \(g\), we have \(\mu(g\cdot C) = \mu(C)\). Such a measure on \(G\) itself is called <em>the</em> <strong>Haar measure</strong>, whose existence and uniqueness are always guaranteed: </p> <div class=note ><div class=title >Haar&#39;s Theorem</div> <div class=content ><em>For every locally compact Hausdorff topological group \(G\), there always exists a countably additive, non-trivial, left-invariant measure \(\mu\) satisfying additional finiteness and regularity properties. Furthermore, \(\mu\) is unique up to non-zero scalar multiples.</em></div></div> <p>The idea behind the construction of Haar measures is to <em>normalize by the scaling factor of group translations</em>. Here are some examples. </p> <div class=note ><div class=title >Examples</div> <div class=content ><p>Consider the Lie group \(\mathbb R^\times_+\) with the Euclidean topology. Consider the Lebesgue measure \(\lambda\) &#40;\(dx\) in differential notation&#41;. Every \(x \in \mathbb R^\times_+\) acts on a subset \(C\subseteq \mathbb R^\times_+\) and stretches the volume &#40;length since 1-dimensional&#41; by a factor of \(x\): </p> \[\lambda{x\cdot C} = x\lambda(C)\] <p>. Therefore, if we normalize \(dx\) by \(\frac{1}{x}\), then it becomes \(\mathbb R^\times_+\)-invariant. The Haar measure is therefore </p> \[\frac{1}{x}dx.\] <p>This turns out to be a brilliant mnemonic for the definition of the Gamma function, where </p> \[\Gamma(t) = \int_{\mathbb R_+}e^{-s}s^{t-1}dt = \int_{\mathbb R_+}e^{-s}s^{t}\; \frac{1}{t}dt\] <p>and this is really an integral over the group \(\mathbb R^\times_+\).</p></div></div> <div class=note ><div class=title >Examples</div> <div class=content ><p>Any invertible matrix \(X\) in the general linear group \(GL_n(\mathbb k), \mathbb k = \mathbb R \text{ or }\mathbb C\) streches volume of \(\mathbb R^{n\times n}\) by \(|\det X|^n\). Therefore, the Haar measure is </p> \[\frac{1}{|\det X|^n}dX.\]</div></div> <div class=note ><div class=title >Examples</div> <div class=content >Any rigid motion in \(SE(n)\) does not stretch the volume. Therefore, the Haar measure agrees with the usual Lebesgue measure&#33;</div></div> <div class=note ><div class=title >Examples</div> <div class=content ><p>The <em>left</em>-invariance can be an important point, since a group can have left- and right-invariant measure not agreeing up to scaling. For example, the group of matrices of the form </p> \[\begin{pmatrix} x & y \\ 0 & 1\end{pmatrix}\] <p>with \(x>0\) has left-invariant measure \(\frac{1}{x^2}dxdy\) and right-invariant measure \(\frac{1}{x}dxdy\), and they are obviously different. </p> <p>Left and right-invariant measures agree for <strong>unimodular groups</strong>, which includes all of compact, discrete,abelian, connected and semisimple, connected and reductive, connected and nilpotent. <a href="https://math.berkeley.edu/~cjdowd/haar1.pdf">BerkeleyNotes</a> offers a very nice exposition.</p></div></div> <h4 id=the_peter-weyl_theorem_and_harmonics ><a href="#the_peter-weyl_theorem_and_harmonics" class=header-anchor >The Peter-Weyl Theorem and Harmonics</a></h4> <p>Now, for a homogeneous space \(X \cong G/H\), existence of the Haar measure is a bit more tricky, since it depends on something called the <em>modular function</em> of the Lie group \(G\) and its restriction to \(H\). Fortunately, all \(X\) in our consideration admits a unique Haar measure, say \(d\mu\), which we may use to define an &#40;Hermitian&#41; inner product on the space of \(\mathbb R\) &#40;resp. \(\mathbb C\)&#41;-valued functions by setting </p> \[\langle f, g\rangle_X:= \int_X fgd\mu.\] <p>&#40;The \(g\) needs conjugation for the case \(\mathbb k = \mathbb C\).&#41; With that comes a notion of the \(L_2\) norm and the space \(L_2(X)\), i.e. the <strong>Hilbert space of square integrable functions</strong>. </p> <p>Since \(G\) acts on \(X\), this action naturally extends to an action on \(L_2(X)\) where \(g\) acts by transforming the domain of \(f\in L_2(X)\). More precisely, we have the <strong>left-regular representation</strong> \(\mathcal{L}\) of \(G\) on \(L_2(X)\), where \(g\) operates by </p> \[g\cdot f(x) := f(g^{-1}x).\] <p>We take put inverse of \(g\) so as to make this a <em>left</em>-action. Verify this on your own&#33;</p> <p>The left-regular representation is particularly important because it is <em>faithful</em> &#40;respects all symmetries from \(G\)&#41; and <em>unitary</em> &#40;respects the inner product&#41;. In fact, it is a fundamental result in harnomic analysis that such unitary representations on spaces of functions completely decomposes into spaces of <em>harmonics</em>. This is the famous <strong>Peter-Weyl Theorem</strong>, aka. Maschke&#39;s theorem for harmonic analysts.</p> <div class=note ><div class=title >The Peter-Weyl Theorem</div> <div class=content ><em>Let \(\rho\) be a unitary complex representation of a compact group \(G\) on a complex Hilbert space \(H\). Then \(H\) splits into an orthogonal direct sum of irreducible finite-dimensional unitary representations of \(G\).</em></div></div> <p>Basis elements for each of the irreducible pieces are called <strong>harmonics</strong>, and the term is justified through the following examples.</p> <div class=note ><div class=title >Examples</div> <div class=content ><p>Let \(G = U(1)\cong SO(2)\cong S^1\). Then, </p> \[L_2(G) = \bigoplus_{k\in \mathbb{Z}} \mathbb C \cdot Y_k, \quad Y_k(\theta) = e^{i\theta/k}.\] <p>This captures the familiar <strong>Fourier transform</strong> between \(S^1\) and its <strong>group of complex characters</strong> \(\mathbb{Z}\) &#40;aka. <strong>frequencies</strong>&#41;, and the basis functions \(Y_k(\theta) = e^{i\theta/k}\) are the <strong>circular harmonics.</strong> Taking the real and imaginary parts, we recover the familiar \(\cos\) and \(\sin\) bases for Fourier series.</p></div></div> <p>The circular harmonis are called <strong>steerable</strong> in the deep learning literature because they can be &quot;steered&quot; under rotations from \(S^1\), and the action of \(S^1\) on a signal \(f\in L_2(S^1)\) decomposes into steering circular harmonics at different frequencies. </p> <p>TODO: add circular harmonics picture.</p> <div class=note ><div class=title >Examples</div> <div class=content ><p>Let \(X = S^2 \cong SO(3)/SO(2)\) be the \(2\)-sphere, with affine coordinates \((\theta, \varphi)\). \(S^2\) has Haar measure \(\sin(\theta)d\varphi d\theta\). Then, we have the decomposition </p> \[L_2(S^2) = \bigoplus_{l \in \mathbb{N}}\bigoplus_{m = -l}^l \mathbb C\cdot Y_l^m\] <p>where </p> \[Y_l^m(\theta, \varphi) = (-1)^m \sqrt{\frac{(2 \ell+1)}{4 \pi} \frac{(\ell-m)!}{(\ell+m)!}} P_{\ell}^m(\cos \theta) e^{i m \varphi}\] <p>are the famous <strong>spherical harmonics.</strong> The \(P_l^m\) are the Legendre polynomials. Every signal \(f\in L_2(S^2)\) is then a sum of spherical harmonics, and the left-regular representation decomposes into rotating each basis harmonic in 3-space.</p></div></div> <p>In fact, the spherical harmonics correspond to the famous electron orbits. </p> <p>TODO: add spherical harmonics electron orbit picture.</p> <p>The orthogonal basis for \(L_2(X)\) obtained via Peter-Weyl is often referred to as the <strong>steerable basis</strong> in the literature. </p> <h3 id=g-convolutions_and_architecture_of_g-cnns ><a href="#g-convolutions_and_architecture_of_g-cnns" class=header-anchor >\(G\)-Convolutions and Architecture of \(G\)-CNNs</a></h3> <p>Say \(X \cong G/H\) for a normal subgroup \(H\), with <em>Haar measure</em> \(d\mu\). We fix a representation \(\rho\) of \(G\) on \(L_2(X)\) with inner product \(\langle -,-\rangle_X\). For a point \(x \in X\), pick a coset representative \(g_x\in G\) such that \(x = [g_xH]\). Then, the <strong>\(G\)-convolution</strong> \(\mathcal{K}_\rho: L_2(X) \to L_2(X)\) is defined as </p> \[(\mathcal{K}_\rho f)(x) := \langle \rho(g_x)(\kappa),f\rangle_X = \int_X (g_x\cdot \kappa)fd\mu.\] <p>More generally, if \(X\cong G/H_1\) and \(Y \cong G/H_2\) are homogeneous spaces of \(G\), then for any \(y\in Y\), pick coset representative \(g_y\) such that \(y = [g_yH_2]\). The \(G\)-convolution \(\mathcal{K}: L_2(X)\to L_2(Y)\) is then </p> \[(\mathcal{K}_\rho f)(y) := \langle \rho(g_y)(\kappa),f\rangle_X = \int_X (g_y\cdot \kappa)fd\mu.\] <p>Homework: verify that \(G\)-convolutions are indeed equivariant&#33;</p> <div class=note ><div class=title >Example</div> <div class=content ><p>The usual convolution </p> \[\kappa\ast f(x) = \int_{y\in\mathbb R^n}\kappa(x-y)f(y)dy\] <p>is in fact a group convolution where \(G = \mathbb R^n\) itself. The term \(-y\) in the argument of \(\kappa\) is precisely the left-regular representation \(\mathcal{L}_y\), where \(y\) acts on \(x\in R^n\) by its inverse \(-y\)&#33;</p></div></div> <p>The most important examples for us will be the left-regular representation \(\mathcal{L}\) and convolutions going between \(L_2(X)\to L_2(X)\), \(L_2(X)\to L_2(G),\) and \(L_2(G)\to L_2(X)\), as we will see in a moment. </p> <p>We remember that our goal is to build convolutional layers that are equivariant under group translations. Namely, we would like to find equivariant transformations \(\mathcal{K}: L_2(X)\to L_2(Y)\). This is the content of <em>Bekkers ICLR 2020, Thm. 1</em>: </p> <div class=note ><div class=title >Theorem <strong>&#40;Group convolution is all you need&#33;&#41;</strong></div> <div class=content ><p><em>Let \(\mathcal{K}: L_2(X) \rightarrow L_2(Y)\) map between signals on homogeneous spaces of \(G\). Let homogeneous space \(Y \cong G / H_2\) such that \(H_2=\operatorname{Stab}_G\left(y_0\right)\) for some chosen origin \(y_0 \in Y\) and let \(g_y \in G\) such that \(\forall_{y \in Y}: y=g_y y_0\). Fix the left-regular representations on \(L_2(X)\) and \(L_2(Y)\).</em> </p> <p><em>Then \(\mathcal{K}\) is equivariant to group \(G\) if and only if:</em></p> <ol> <li><p><em>It is a group convolution \(\mathcal{K}[\kappa]\): \((\mathcal{K} f)(y)=\int_X \kappa\left(g_y^{-1} x\right) f(x) \mathrm{d}\mu(x)\).</em></p> <li><p><em>The kernel is \(H_2\)-invariant: \(\kappa(h^{-1} x)=\kappa(x)\) for all \(h\in H_2\).</em></p> </ol></div></div> <blockquote> <p>TODO: add proof</p> </blockquote> <p>Furthermore, let the big group \(G \cong X\rtimes H\) be a semidirect product and \(X = Y \cong G/H\). Any such \(H\)-invariant kernel \(\kappa\) comes from a <strong>lift</strong> \(\hat{\kappa}\in L_2(G)\) on the big group, without additional symmetry constraints.</p> <div class=note ><div class=title >Theorem</div> <div class=content ><p>Let \(X,G\) be as above, and \(\kappa\in L_2(X)\) be \(H\)-invariant: \(\kappa(h^{-1} x)=\kappa(x)\) for all \(h\in H\). Then, there is another kernel \(\hat{\kappa}\in L_2(G)\) and convolutions \(\mathcal{K}_{X\to G}: L_2(X)\to L_2(G)\), \(\mathcal{K}_{G\to X}:L_2(G)\to L_2(X)\) such that </p> \[\mathcal{K}[\kappa] = \mathcal{K}_{G\to X}\circ \mathcal{K}[\hat{\kappa}]\circ \mathcal{K}_{X\to G}.\] <p>Conversely, any kernel \(\hat{\kappa}\in L_2(G)\) gives rise to an \(H\)-invariant kernel \(\kappa\) via the same process.</p></div></div> \[\begin{tikzcd} {L_2(G)} & {L_2(G)} \\ {L_2(X)} & {L_2(X)} \arrow["{\hat{\kappa}}"{description}, from=1-1, to=1-2] \arrow["{\text{ pooling }}"{description}, from=1-2, to=2-2] \arrow["{\text{ lifting }}"{description}, from=2-1, to=1-1] \arrow["\kappa"{description}, from=2-1, to=2-2] \end{tikzcd}\] <p>The convolution \(\mathcal{K}_{X\to G}\) is called the <strong>lifting convolution</strong>, which takes in a signal \(f\in L_2(X)\) and outputs </p> \[\hat{f} := \mathcal{K}_{X\to G}[\kappa](f) \in L_2(G), \quad \hat{f}(x, h) = \int_{y \in X}\kappa(h^{-1}(y-x))f(y)d\mu(y).\] <p>The convolution \(\mathcal{K}_{G\to X}\) is given by mean pooling over \(H\). </p> <p>This concludes the scaffolding of our \(G\)-CNN architecture. Since the group \(G\) for all practical purposes will be a semidirect product of \(\mathbb R^n\) with some of its symmetries, our convolutional layers can always be decomposed into a lifting convolution from \(\mathbb R^n\) to \(G\), followed by an unconstrained convolution on \(G\), followed by a pooling convolution from \(G\) back to \(\mathbb R^n\). Since we used left-regular representations throughout, such an architecture is called a <strong>regular</strong> \(G\)-convolutional network.</p> <h2 id=g-cnn_regular_vs_steerable_networks ><a href="#g-cnn_regular_vs_steerable_networks" class=header-anchor >\(G\)-CNN: Regular v.s. Steerable Networks</a></h2> <p>So we need to lift any signal \(f\) on \(X\) &#40;usually \(X = \mathbb R^n\)&#41; to a signal on the big group \(G\), and this is done via the lifting convolution, where we lift the kernel \(\kappa\) to \(\hat{\kappa}\in L_2(G)\). This theoretically gives us true \(G\)-equivariance and replicates every signal to infinite precision. However, as with any other computer program, we need to choose a suitable discretization scheme. </p> <p>TODO: add schematic of lifting convolution</p> <p>Schematically, we have infinitely many copies of \(X\) in \(G = X\rtimes H\) indexed by \(H\), represented as the horizontal planes. These are the <strong>sections</strong> of \(X\) in \(G\). If we fix one such horizontal plane, then we see that there is a copy of \(H\) sticking out vertically of each point on the plane. These copies of \(H\) are called the <strong>fibers</strong> of the projection map \(G\to X\). It is then a question of whether to discritize <em>horizontally</em> or <em>vertically</em>. </p> <h3 id=regular_g-cnns ><a href="#regular_g-cnns" class=header-anchor >Regular \(G\)-CNNs</a></h3> <p>Say we want to slice a feature map on \(G\) by horizontal sections. Then, each horizontal slice is just a feature map on \(X\) itself, and traversing vertically through different copies of \(X\) amounts to transforming \(X\) under the action of \(H\). For example, if we slice up a kernel \(\hat{\kappa}\) on \(SE(n)\), then each slice \(\kappa\) is just a kernel on \(\mathbb R^n\) rotated from some original position by a rotation in \(SO(n)\). Discretizing in this direction then amounts to choosing finitely many transformations in \(H\) to approximate all of \(H\). </p> <div class=note ><div class=title >Example</div> <div class=content >If \(G = SE(2)\), then we may discretize \(H = SO(2) \cong S^1\) by sampling at the \(k^{\text{th}}\) roots of unity. This corresponds to choosing a kernel \(\kappa \in L_2(\mathbb R^2)\) and rotating it \(k\) times at angles \(2\pi/k\).</div></div> <p>The regular \(G\)-CNN architecture loses true equivariance, because the kernel is not transformed via all elements in \(H\). This approach resembles the traditional method of <strong>data augmentation</strong>, where the input signal is transformed by a discrete subgroup of \(H\) and then all fed into a non-equivariant convolutonal layer. The fundamental improvement here is that regular \(G\)-CNNs are able to capture <strong>local transformations</strong> because the kernel is transformed, while data augmentation methods only captures <strong>global transformations</strong>.</p> <p>TODO: add schematic of local vs. global.</p> <h3 id=steerable_g-cnns ><a href="#steerable_g-cnns" class=header-anchor >Steerable \(G\)-CNNs</a></h3> <p>So let&#39;s try slicing feature maps the other way, by vertical fibers. Then, the feature map <strong>remains on \(X\)</strong> but its <strong>values are signals</strong> on the fibers \(H\) over each point \(x\in X\). &#40;This is essentially the idea of currying.&#41;</p> <p>Formally, given some signal \(f\in L_2(G)\), we view it as a two-variable function \(f(x,h)\) for \(x\in X, h\in H\) and curry it: </p> \[f_x: = f(x, -): H\to \mathbb R \quad \text{ for each fixed } x\in X.\] <p>We have an &quot;augmented&quot; feature map </p> \[F: X \to L_2(H), \quad x\mapsto f_x,\] <p>and Peter-Weyl comes in handy: we may decompose each signal on \(H\) as <strong>a sum of the</strong> \(H\)-<strong>harmonics</strong>, and discretizing this signal amounts to <strong>choosing a finite bandwidth</strong> &#40;i.e. storing the Fourier coefficients in front of a finite number of frequencies. Since we are decomposing the fiber-wise signals into the steerable \(H\)-basis, this architecture is referred to as <strong>steerable \(G\)-CNN</strong>.</p> <div class=note ><div class=title >Example</div> <div class=content ><p>For \(G = SE(2)\), we get a feature map on \(\mathbb R^2\) of Fourier coefficients in front of the <strong>cicular harmonics</strong>. </p> <p>For \(G = SE(3)\), we get a feature map on \(\mathbb R^3\) of Fourier coefficients in front of the spherical harmonics. This is essentially the content of the paper <a href="https://rdcu.be/bQuEQ"><em>Esteves ECCV 2018</em></a>.</p></div></div> <p>TODO: add screenshots from the papers.</p> <p>The steerable architecture achieves <strong>true equivariance</strong>, but the choice of bandwidth limits the amount of precision that the feature map is able to approximate a signal. As with Taylor and Fourier series, we can always raise the precision by choosing a bigger bandwidth &#40;allowing more basis harmonics in the finite sum approximation&#41;. </p> <h3 id=g-activation_layers ><a href="#g-activation_layers" class=header-anchor >\(G\)-Activation Layers</a></h3> <p>Finally, there is the issue that activation layers do not yet have built-in equivariance. Again, there are two approaches to build equivariant activation layers. </p> <p>First, we may simply choose equivariant activation functions. </p> <div class=note ><div class=title >Example</div> <div class=content >Even though the usual RELU function is not equivariant to rotations on the nose, we can postcompose it to the norm function. Since the norm is rotation-invariant, the resulting norm-RELU function becomes rotation-invariant. However, this comes at a cost of loosing all the directional information.</div></div> <p>The more preferable choice is adhering to the steerable framework and apply activation functions <strong>fiber-wise</strong>. Since the signals on each fiber \(H\) is steerable, it remains steerable even if we cut off a portion of if by say a RELU. This allows basically any familiar non-linear activation function but can create <em>sharp corners</em> which requires a bigger bandwidth to approximate.</p> <p>TODO: add steerable RELU picture</p> <h2 id=references ><a href="#references" class=header-anchor >References</a></h2> <p><em>Prof.</em> <a href="https://www.cis.upenn.edu/~jean/home.html">Jean Gallier</a> at Penn has published a number of amazing volumes on all of the representation theory, differential geometry, and harmonic analysis useful for the above discussions. The ones particularly relevant are: </p> <ul> <li><p><em>Aspects of Representation Theory and Noncommutative Harmonic Analysis</em></p> <li><p><em>Geometric Methods and Applications For Computer Science and Engineering</em></p> <li><p><em>Differential Geometry and Lie Groups: A Second Course</em></p> </ul> <p>The author of this blog owes immense gratitude to <em>Prof.</em> Gallier for introducing him to this subject during his undergraduate and offering incredibly helpful advice. </p> <p><a href="https://chen-cai-osu.github.io/">Chen Cai</a> has created a great repository of papers on equivariant networks: &#40;Awesome Equivariant Network&#41;&#91;https://github.com/Chen-Cai-OSU/awesome-equivariant-network&#93;. </p> <p>See &#40;Geometric Deep Learning&#41;&#91;https://geometricdeeplearning.com/&#93; for a comprehensive treatment of geometric methods used in deep learning.</p> <div class=page-foot > Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. <a href="https://www.flaticon.com/free-icons/machine-learning" title="machine learning icons">Machine learning icons created by Freepik - Flaticon</a> </div> </div> </div> <script src="/Neural-Networks-with-Math/libs/katex/katex.min.js"></script> <script src="/Neural-Networks-with-Math/libs/katex/contrib/auto-render.min.js"></script> <script>renderMathInElement(document.body)</script>