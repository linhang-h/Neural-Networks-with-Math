<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/Neural-Networks-with-Math/libs/katex/katex.min.css"> <link rel=stylesheet  href="/Neural-Networks-with-Math/css/franklin.css"> <link rel=stylesheet  href="/Neural-Networks-with-Math/css/style.css"> <link rel=icon  type="image/png" sizes=32x32  href="/Neural-Networks-with-Math/assets/images/network.png"> <title>Introduction to G-CNNs</title> <header> <nav> <div class=nav-content > <div class=logo > <a href="/Neural-Networks-with-Math/">Neural Networks with Math</a> </div> <ul class=nav-links > <li><a href="/Neural-Networks-with-Math/">Home</a> <li><a href="/Neural-Networks-with-Math/menu1">Posts</a> <li><a href="/Neural-Networks-with-Math/people">People</a> </ul> </div> </nav> <script> let nav = document.querySelector("nav"); window.onscroll = function() { if(document.documentElement.scrollTop > 20){ nav.classList.add("sticky"); }else { nav.classList.remove("sticky"); } } </script> </header> <div class="container py-3 px-3 mx-auto"><div class=franklin-content > <h1 id=group_equivariant_neural_networks ><a href="#group_equivariant_neural_networks" class=header-anchor >Group Equivariant Neural Networks</a></h1> <p>This webpage consists of a set of notes we created while going through the course <em>An Introduction to Group Equivariant Deep Learning</em> by Erik Bekkers at the University of Amsterdam. The course materials are freely available online at <a href="https://uvagedl.github.io/">uvagedl</a>, and the lecture series is available as a public <a href="https://www.youtube.com/playlist?list&#61;PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd">playlist</a> on YouTube. We owe our deepest gratitude for <em>Dr.</em> Erik Bekkers and all other people who created this wonderful course and made the materials publically available. We largely follow the structure of these lectures, even though some of the mathematical details are our own takes on the original arguments. The intrigued reader is highly recommended to check out the original lectures, where concepts are illustrated with many amazing graphics. We hope to duely justify the slogan &#39;<em>Group convolution is all you need</em>&#39; through this short digest. </p> <div class=franklin-toc ><ol><li><a href="#introduction">Introduction</a><li><a href="#g-cnn_the_first_class">\(G\)-CNN: The First Class</a><ol><li><a href="#convolutional_neural_networks">Convolutional Neural Networks</a><li><a href="#group_equivariant_cnns">Group Equivariant CNNs</a></ol><li><a href="#representation_theory_and_harmonic_analysis_on_locally_compact_groups">Representation Theory and Harmonic Analysis on Locally Compact Groups</a><li><a href="#g-cnn_regular_vs_steerable_networks">\(G\)-CNN: Regular v.s. Steerable Networks</a><li><a href="#g-gnn_equivariant_graph_neural_networks">\(G\)-GNN: Equivariant Graph Neural Networks</a></ol></div> <h2 id=introduction ><a href="#introduction" class=header-anchor >Introduction</a></h2> <p>Convolutional Neural Networks &#40;CNNs&#41; possess the distinguishing property of translation invariance in their convolutional layers, allowing them to maintain the inherent spatial structure of image data. This characteristic enables CNNs to excel in various complex tasks, including edge detection, feature extraction for object recognition, and semantic segmentation.</p> <p>However, image data often come with additional structures, such as rotational or reflectional symmetries, which are not built-in symmetries of traditional CNNs. Group-equivariant CNNs &#40;\(G\)-CNNs&#41; extend the translation-invariance property of traditional CNNs into equivariance of more general symmetry groups, such as rotations, reflections, or scaling transformations. This is achieved by defining convolutional layers over topological groups rather than the usual vector space \(\mathbb{R}^n\). As a result, \(G\)-CNNs efficiently represents and processes data with a lot of symmetries, in many cases improving the performance margin significantly. One of the first applications where \(G\)-CNNs are extremely useful is medical imaging, where cells and organs can appear in various orientations &#40;&#91;Bekkers et.al. 2018&#93;&#41;. </p> <p>&#33;&#33;&#33;TBA</p> <h2 id=g-cnn_the_first_class ><a href="#g-cnn_the_first_class" class=header-anchor >\(G\)-CNN: The First Class</a></h2> <p>Say we want to build a network which recognizes the letter \(G\). Regardless of where we position the \(G\) in our input image, we want the feature map of the network to activate in the same way where it detects the \(G\) somewhere in the image. This sort of <em>translation equivariance</em> is a defining feature of Convolutional Neural Networks &#40;CNNs&#41;, and in fact what makes it so useful in pattern recognition tasks, especially in image and spatial data. </p> <h3 id=convolutional_neural_networks ><a href="#convolutional_neural_networks" class=header-anchor >Convolutional Neural Networks</a></h3> <p>CNNs achieve translation invariance through the structure of their convolutional layers. Mathematically, a convolutional layer takes in a signal \(f:\mathbb{R}^n\to \mathbb{R}\) and computes a feature map \(\hat{f}(x)\) as </p> \[\hat{f}(x):=(\kappa * f)(x) = \int_{\mathbb{R}^n} \kappa(y) f(x - y) \, dy\] <p>, where \(\kappa\) is the convolution <em>kernel</em> &#40;aka. filter&#41;. If the signal \(f\) is translated by a vector \(t \in \mathbb{R}^n\), say \(f^\prime(x) = f(x - t)\), then the resulting feature map shifts correspondingly: </p> \[\hat{f}^\prime(x) = (\kappa * f')(x) = \int_{\mathbb{R}^n}\kappa(y)f(x-t-y)dy = \hat{f}(x-t).\] <p>This can be organized into a commutative diagram: </p> \[\begin{tikzcd} f & {\hat{f}} \\ {f^\prime} & {\hat{f}^\prime} \arrow["{\tiny{\text{ convolution }}}"{marking, allow upside down}, from=1-1, to=1-2] \arrow["{\tiny{\text{ translation }}}"{marking, allow upside down}, from=1-1, to=2-1] \arrow["{\tiny{\text{ translation }}}"{marking, allow upside down}, from=1-2, to=2-2] \arrow["{\tiny{\text{ convolution }}}"{marking, allow upside down}, from=2-1, to=2-2] \end{tikzcd}\] <p>In plain words, <em>convolving first and then translating results the same as translating first and then convolving</em>. This demonstrates that the convolution operation is <em>compatible with translational symmetries</em> and is exactly what makes CNNs so good with acoustic and visual signals. </p> <h3 id=group_equivariant_cnns ><a href="#group_equivariant_cnns" class=header-anchor >Group Equivariant CNNs</a></h3> <p>Our physical world is brimming with symmetries – from the discrete polytopal symmetries of a virus to the rotational symmetries of the surface of a planet. </p> <p><em>TODO: add pictures of the icosahedral and SO&#40;3&#41; groups</em></p> <p>It is an inevitable consequence that the data inputted in neural networks come with a lot of built-in symmetries. Respecting, or better, leveraging those symmetries has proven to be the key in speeding up training and improving accuracy. It is therefore requisite that we build network architectures that are <em>compatible with more general symmetries</em>, i.e. groups other than \(\mathbb R^n\). </p> <p>To do this, we ask the generalized convolution layers to satisfy a similar property: </p> <p>&lt;p style&#61;&quot;text-align: center;&quot;&gt; Group convolution followed by group translation should be the same as group translation followed by group convolution. &lt;/p&gt;</p> \[\begin{tikzcd} f & {\hat{f}} \\ {f^\prime} & {\hat{f}^\prime} \arrow["{\tiny{G-\text{ convolution }}}"{marking, allow upside down}, from=1-1, to=1-2] \arrow["{\tiny{G-\text{ translation }}}"{marking, allow upside down}, from=1-1, to=2-1] \arrow["{\tiny{G-\text{ translation }}}"{marking, allow upside down}, from=1-2, to=2-2] \arrow["{\tiny{G-\text{ convolution }}}"{marking, allow upside down}, from=2-1, to=2-2] \end{tikzcd}\] <p>Fix the ground field to be \(\mathbb k\), which for all practical purposes will be \(\mathbb R\) or \(\mathbb C\). &#40;It will be awesome if you are able to come up with an application of \(G\)-CNNs with a more exotic ground field&#33;&#41;</p> <p>So say we are given a function \(f\) on some space \(X\) with a large group of symmetries \(G\) –– so large that \(X\) is in fact a <em>homogeneous space</em> of \(G\). &#40;This requirement is so that we have nice representation theory of \(G\) on the space of functions over \(X\).&#41; A \(G\)-convolution then amounts to a choice of convolution kernel \(\kappa: X \to \mathbb k\) </p> <div class=note ><div class=title >Theorem</div> <div class=content >Content of the theorem.</div></div> <h2 id=representation_theory_and_harmonic_analysis_on_locally_compact_groups ><a href="#representation_theory_and_harmonic_analysis_on_locally_compact_groups" class=header-anchor >Representation Theory and Harmonic Analysis on Locally Compact Groups</a></h2> <h2 id=g-cnn_regular_vs_steerable_networks ><a href="#g-cnn_regular_vs_steerable_networks" class=header-anchor >\(G\)-CNN: Regular v.s. Steerable Networks</a></h2> <h2 id=g-gnn_equivariant_graph_neural_networks ><a href="#g-gnn_equivariant_graph_neural_networks" class=header-anchor >\(G\)-GNN: Equivariant Graph Neural Networks</a></h2> <script> var acc = document.getElementsByClassName("theorem-accordion"); var i; for (i = 0; i < acc.length; i++) { acc[i].addEventListener("click", function() { this.classList.toggle("active"); var panel = this.nextElementSibling; if (panel.style.maxHeight) { panel.style.maxHeight = null; } else { panel.style.maxHeight = panel.scrollHeight + "px"; } }); } </script> <div class=page-foot > Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. <a href="https://www.flaticon.com/free-icons/machine-learning" title="machine learning icons">Machine learning icons created by Freepik - Flaticon</a> </div> </div> </div> <script src="/Neural-Networks-with-Math/libs/katex/katex.min.js"></script> <script src="/Neural-Networks-with-Math/libs/katex/contrib/auto-render.min.js"></script> <script>renderMathInElement(document.body)</script>